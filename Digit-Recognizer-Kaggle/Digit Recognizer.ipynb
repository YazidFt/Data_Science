{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras\n",
    "from keras.layers import Dense, Conv2D, MaxPooling2D, Activation, Flatten\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the training set\n",
    "X_train = np.load('MNIST/train_data.npy')\n",
    "y_train = np.load('MNIST/train_labels.npy')\n",
    "## Load the testing set\n",
    "X_test = np.load('MNIST/test_data.npy')\n",
    "y_test = np.load('MNIST/test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val, y_val\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  5\n"
     ]
    }
   ],
   "source": [
    "plt.imshow(X_train[1,].reshape(28,28), cmap=\"Greys\")\n",
    "plt.show()\n",
    "print(\"Label: \", y_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will train a linear classifier $\\vec{x} \\rightarrow y$ with SGD using TensorFlow.\n",
    "\n",
    "We  need to calculate a logit $z_k$ for each class: \n",
    "$$z_k = \\vec{x} \\cdot \\vec{w_k} + b_k \\quad k = 0..9$$\n",
    "\n",
    "And transform logits $z_k$ to valid probabilities $p_k$ with softmax: \n",
    "$$p_k = \\frac{e^{z_k}}{\\sum_{i=0}^{9}{e^{z_i}}} \\quad k = 0..9$$\n",
    "\n",
    "We will use a cross-entropy loss to train our multi-class classifier:\n",
    "$$\\text{cross-entropy}(y, p) = -\\sum_{k=0}^{9}{\\log(p_k)[y = k]}$$ \n",
    "\n",
    "where \n",
    "$$\n",
    "[x]=\\begin{cases}\n",
    "       1, \\quad \\text{if $x$ is true} \\\\\n",
    "       0, \\quad \\text{otherwise}\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "Cross-entropy minimization pushes $p_k$ close to 1 when $y = k$, which is what we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Labels to One-hot-encoding\n",
    "y_train_hot = keras.utils.to_categorical(y_train, 10)\n",
    "y_val_hot = keras.utils.to_categorical(y_val, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization:\n",
    "#X_train = X_train/255 - 0.5 [for 0 centred data]\n",
    "#X_val = X_val/255 - 0.5h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0917 16:12:17.998793  7224 deprecation.py:506] From c:\\users\\blender\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "#Variables\n",
    "W = tf.get_variable(\"x\", shape=(784,10), dtype=tf.float32) \n",
    "b = tf.get_variable(\"b\", shape=(1,10), dtype=tf.float32)\n",
    "\n",
    "input_X = tf.placeholder(tf.float32, shape=(None,784))\n",
    "input_y = tf.placeholder(tf.int32, shape=(None,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute softmax predictions & classes:\n",
    "logits = input_X @ W + b\n",
    "#probabilities\n",
    "probas = tf.nn.softmax(logits)\n",
    "#classes\n",
    "classes = tf.argmax(probas, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss and Optimization:\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=input_y, logits=logits))\n",
    "opt = tf.train.AdamOptimizer().minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "EPOCHS = 50\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: train_loss:  47.2523, val_loss: 32.7755, train_accuracy: 0.5517, val_accuracy: 0.5333\n",
      "EPOCH 1: train_loss:  24.3874, val_loss: 22.1527, train_accuracy: 0.7013, val_accuracy: 0.6800\n",
      "EPOCH 2: train_loss:  16.9595, val_loss: 17.3389, train_accuracy: 0.7583, val_accuracy: 0.7242\n",
      "EPOCH 3: train_loss:  13.2176, val_loss: 14.7703, train_accuracy: 0.7971, val_accuracy: 0.7625\n",
      "EPOCH 4: train_loss:  10.9839, val_loss: 13.1396, train_accuracy: 0.8194, val_accuracy: 0.7758\n",
      "EPOCH 5: train_loss:  9.4354, val_loss: 12.2881, train_accuracy: 0.8363, val_accuracy: 0.7925\n",
      "EPOCH 6: train_loss:  8.2774, val_loss: 11.5683, train_accuracy: 0.8462, val_accuracy: 0.8050\n",
      "EPOCH 7: train_loss:  7.4109, val_loss: 11.0483, train_accuracy: 0.8575, val_accuracy: 0.8083\n",
      "EPOCH 8: train_loss:  6.7023, val_loss: 10.6469, train_accuracy: 0.8642, val_accuracy: 0.8192\n",
      "EPOCH 9: train_loss:  6.1200, val_loss: 10.3153, train_accuracy: 0.8698, val_accuracy: 0.8233\n",
      "EPOCH 10: train_loss:  5.5931, val_loss: 10.0979, train_accuracy: 0.8765, val_accuracy: 0.8267\n",
      "EPOCH 11: train_loss:  5.1425, val_loss: 9.7291, train_accuracy: 0.8760, val_accuracy: 0.8283\n",
      "EPOCH 12: train_loss:  4.7514, val_loss: 9.4998, train_accuracy: 0.8798, val_accuracy: 0.8308\n",
      "EPOCH 13: train_loss:  4.4095, val_loss: 9.2787, train_accuracy: 0.8852, val_accuracy: 0.8258\n",
      "EPOCH 14: train_loss:  4.1216, val_loss: 9.0598, train_accuracy: 0.8894, val_accuracy: 0.8308\n",
      "EPOCH 15: train_loss:  3.8383, val_loss: 8.9007, train_accuracy: 0.8990, val_accuracy: 0.8317\n",
      "EPOCH 16: train_loss:  3.6106, val_loss: 8.8387, train_accuracy: 0.9000, val_accuracy: 0.8250\n",
      "EPOCH 17: train_loss:  3.3393, val_loss: 8.6951, train_accuracy: 0.9000, val_accuracy: 0.8283\n",
      "EPOCH 18: train_loss:  3.1984, val_loss: 8.6621, train_accuracy: 0.9000, val_accuracy: 0.8350\n",
      "EPOCH 19: train_loss:  3.0856, val_loss: 8.4944, train_accuracy: 0.8990, val_accuracy: 0.8350\n",
      "EPOCH 20: train_loss:  2.9516, val_loss: 8.3181, train_accuracy: 0.9044, val_accuracy: 0.8333\n",
      "EPOCH 21: train_loss:  2.7589, val_loss: 8.3324, train_accuracy: 0.9100, val_accuracy: 0.8333\n",
      "EPOCH 22: train_loss:  2.5336, val_loss: 8.2095, train_accuracy: 0.9071, val_accuracy: 0.8300\n",
      "EPOCH 23: train_loss:  2.5556, val_loss: 7.9754, train_accuracy: 0.9225, val_accuracy: 0.8400\n",
      "EPOCH 24: train_loss:  2.3747, val_loss: 8.2320, train_accuracy: 0.9062, val_accuracy: 0.8383\n",
      "EPOCH 25: train_loss:  2.3058, val_loss: 7.9088, train_accuracy: 0.9190, val_accuracy: 0.8383\n",
      "EPOCH 26: train_loss:  2.2317, val_loss: 7.8032, train_accuracy: 0.9260, val_accuracy: 0.8408\n",
      "EPOCH 27: train_loss:  2.0414, val_loss: 7.9942, train_accuracy: 0.9183, val_accuracy: 0.8375\n",
      "EPOCH 28: train_loss:  1.9703, val_loss: 7.8924, train_accuracy: 0.9150, val_accuracy: 0.8442\n",
      "EPOCH 29: train_loss:  1.9044, val_loss: 7.6655, train_accuracy: 0.9206, val_accuracy: 0.8483\n",
      "EPOCH 30: train_loss:  1.7815, val_loss: 7.6616, train_accuracy: 0.9246, val_accuracy: 0.8467\n",
      "EPOCH 31: train_loss:  1.7018, val_loss: 7.7228, train_accuracy: 0.9315, val_accuracy: 0.8442\n",
      "EPOCH 32: train_loss:  1.5366, val_loss: 7.6923, train_accuracy: 0.9292, val_accuracy: 0.8375\n",
      "EPOCH 33: train_loss:  1.4751, val_loss: 7.5941, train_accuracy: 0.9285, val_accuracy: 0.8467\n",
      "EPOCH 34: train_loss:  1.4873, val_loss: 7.4261, train_accuracy: 0.9369, val_accuracy: 0.8492\n",
      "EPOCH 35: train_loss:  1.4554, val_loss: 7.5183, train_accuracy: 0.9296, val_accuracy: 0.8508\n",
      "EPOCH 36: train_loss:  1.3390, val_loss: 7.4913, train_accuracy: 0.9354, val_accuracy: 0.8458\n",
      "EPOCH 37: train_loss:  1.2870, val_loss: 7.4192, train_accuracy: 0.9394, val_accuracy: 0.8500\n",
      "EPOCH 38: train_loss:  1.1734, val_loss: 7.4581, train_accuracy: 0.9385, val_accuracy: 0.8408\n",
      "EPOCH 39: train_loss:  1.1575, val_loss: 7.3443, train_accuracy: 0.9433, val_accuracy: 0.8475\n",
      "EPOCH 40: train_loss:  1.1756, val_loss: 7.4536, train_accuracy: 0.9465, val_accuracy: 0.8492\n",
      "EPOCH 41: train_loss:  1.0663, val_loss: 7.3465, train_accuracy: 0.9354, val_accuracy: 0.8483\n",
      "EPOCH 42: train_loss:  1.0334, val_loss: 7.4021, train_accuracy: 0.9377, val_accuracy: 0.8508\n",
      "EPOCH 43: train_loss:  0.9742, val_loss: 7.5587, train_accuracy: 0.9356, val_accuracy: 0.8458\n",
      "EPOCH 44: train_loss:  0.9795, val_loss: 7.3700, train_accuracy: 0.9410, val_accuracy: 0.8425\n",
      "EPOCH 45: train_loss:  0.9715, val_loss: 7.2112, train_accuracy: 0.9558, val_accuracy: 0.8483\n",
      "EPOCH 46: train_loss:  0.9324, val_loss: 7.3789, train_accuracy: 0.9452, val_accuracy: 0.8400\n",
      "EPOCH 47: train_loss:  0.9132, val_loss: 7.5658, train_accuracy: 0.9300, val_accuracy: 0.8317\n",
      "EPOCH 48: train_loss:  0.9355, val_loss: 7.4856, train_accuracy: 0.9452, val_accuracy: 0.8433\n",
      "EPOCH 49: train_loss:  0.9276, val_loss: 7.4216, train_accuracy: 0.9494, val_accuracy: 0.8483\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    batch_losses=[]\n",
    "    \n",
    "    for i in range(0, X_train.shape[0], BATCH_SIZE):\n",
    "        _, bach_loss = sess.run([opt, loss], feed_dict={input_X: X_train[i:i+BATCH_SIZE],\n",
    "                                                         input_y: y_train_hot[i:i+BATCH_SIZE]})\n",
    "        batch_losses.append(bach_loss)\n",
    "    \n",
    "    #Train,Val losses\n",
    "    train_loss = np.mean(batch_losses)\n",
    "    val_loss = sess.run(loss,  feed_dict={input_X: X_val, input_y: y_val_hot}) \n",
    "    #Accuracy\n",
    "    train_accuracy = accuracy_score(y_train, sess.run(classes, {input_X: X_train}))\n",
    "    val_accuracy = accuracy_score(y_val, sess.run(classes, {input_X: X_val}))\n",
    "    #print results\n",
    "    print(\"EPOCH \"+str(epoch)+\": train_loss: {0: .4f}, val_loss: {1:.4f}, train_accuracy: {2:.4f}, val_accuracy: {3:.4f}\"\n",
    "          .format(train_loss, val_loss, train_accuracy, val_accuracy))\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net: 91% in validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_NN():\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(128, input_shape=(784,)))\n",
    "    model1.add(Activation('sigmoid'))\n",
    "    model1.add(Dense(128))\n",
    "    model1.add(Activation('sigmoid'))\n",
    "    model1.add(Dense(128))\n",
    "    model1.add(Activation('sigmoid'))\n",
    "    model1.add(Dense(10))\n",
    "    model1.add(Activation('softmax'))\n",
    "    return model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 134,794\n",
      "Trainable params: 134,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1 = create_NN()\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 16:22:16.896091  7224 deprecation_wrapper.py:119] From c:\\users\\blender\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0917 16:22:16.987040  7224 deprecation_wrapper.py:119] From c:\\users\\blender\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model1.compile( loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 1200 samples\n",
      "Epoch 1/40\n",
      "4800/4800 [==============================] - 0s 54us/step - loss: 0.1307 - acc: 0.9671 - val_loss: 0.2751 - val_acc: 0.9167\n",
      "Epoch 2/40\n",
      "4800/4800 [==============================] - 0s 55us/step - loss: 0.1338 - acc: 0.9658 - val_loss: 0.2834 - val_acc: 0.9125\n",
      "Epoch 3/40\n",
      "4800/4800 [==============================] - 0s 57us/step - loss: 0.1330 - acc: 0.9667 - val_loss: 0.2767 - val_acc: 0.9133\n",
      "Epoch 4/40\n",
      "4800/4800 [==============================] - 0s 50us/step - loss: 0.1263 - acc: 0.9669 - val_loss: 0.2864 - val_acc: 0.9183\n",
      "Epoch 5/40\n",
      "4800/4800 [==============================] - 0s 50us/step - loss: 0.1195 - acc: 0.9704 - val_loss: 0.2888 - val_acc: 0.9092\n",
      "Epoch 6/40\n",
      "4800/4800 [==============================] - 0s 52us/step - loss: 0.1216 - acc: 0.9708 - val_loss: 0.2800 - val_acc: 0.9125\n",
      "Epoch 7/40\n",
      "4800/4800 [==============================] - 0s 52us/step - loss: 0.1176 - acc: 0.9710 - val_loss: 0.2860 - val_acc: 0.9067\n",
      "Epoch 8/40\n",
      "4800/4800 [==============================] - 0s 52us/step - loss: 0.1089 - acc: 0.9750 - val_loss: 0.2833 - val_acc: 0.9075\n",
      "Epoch 9/40\n",
      "4800/4800 [==============================] - 0s 51us/step - loss: 0.1038 - acc: 0.9750 - val_loss: 0.2753 - val_acc: 0.9133\n",
      "Epoch 10/40\n",
      "4800/4800 [==============================] - 0s 54us/step - loss: 0.1013 - acc: 0.9769 - val_loss: 0.2801 - val_acc: 0.9108\n",
      "Epoch 11/40\n",
      "4800/4800 [==============================] - 0s 51us/step - loss: 0.1006 - acc: 0.9781 - val_loss: 0.2911 - val_acc: 0.9108\n",
      "Epoch 12/40\n",
      "4800/4800 [==============================] - 0s 53us/step - loss: 0.1026 - acc: 0.9773 - val_loss: 0.2966 - val_acc: 0.9142\n",
      "Epoch 13/40\n",
      "4800/4800 [==============================] - 0s 53us/step - loss: 0.0998 - acc: 0.9773 - val_loss: 0.2883 - val_acc: 0.9142\n",
      "Epoch 14/40\n",
      "4800/4800 [==============================] - 0s 55us/step - loss: 0.1028 - acc: 0.9769 - val_loss: 0.2858 - val_acc: 0.9192\n",
      "Epoch 15/40\n",
      "4800/4800 [==============================] - 0s 59us/step - loss: 0.0979 - acc: 0.9790 - val_loss: 0.2786 - val_acc: 0.9175\n",
      "Epoch 16/40\n",
      "4800/4800 [==============================] - 0s 60us/step - loss: 0.0925 - acc: 0.9769 - val_loss: 0.2828 - val_acc: 0.9167\n",
      "Epoch 17/40\n",
      "4800/4800 [==============================] - 0s 59us/step - loss: 0.0860 - acc: 0.9796 - val_loss: 0.2757 - val_acc: 0.9125\n",
      "Epoch 18/40\n",
      "4800/4800 [==============================] - 0s 53us/step - loss: 0.0850 - acc: 0.9779 - val_loss: 0.2904 - val_acc: 0.9150\n",
      "Epoch 19/40\n",
      "4800/4800 [==============================] - 0s 67us/step - loss: 0.0843 - acc: 0.9794 - val_loss: 0.3016 - val_acc: 0.9025\n",
      "Epoch 20/40\n",
      "4800/4800 [==============================] - 0s 61us/step - loss: 0.0798 - acc: 0.9800 - val_loss: 0.2896 - val_acc: 0.9067\n",
      "Epoch 21/40\n",
      "4800/4800 [==============================] - 0s 56us/step - loss: 0.0809 - acc: 0.9800 - val_loss: 0.2887 - val_acc: 0.9158\n",
      "Epoch 22/40\n",
      "4800/4800 [==============================] - 0s 61us/step - loss: 0.0770 - acc: 0.9813 - val_loss: 0.2707 - val_acc: 0.9183\n",
      "Epoch 23/40\n",
      "4800/4800 [==============================] - 0s 56us/step - loss: 0.0729 - acc: 0.9829 - val_loss: 0.2742 - val_acc: 0.9125\n",
      "Epoch 24/40\n",
      "4800/4800 [==============================] - 0s 51us/step - loss: 0.0716 - acc: 0.9831 - val_loss: 0.2721 - val_acc: 0.9158\n",
      "Epoch 25/40\n",
      "4800/4800 [==============================] - 0s 55us/step - loss: 0.0673 - acc: 0.9854 - val_loss: 0.2716 - val_acc: 0.9150\n",
      "Epoch 26/40\n",
      "4800/4800 [==============================] - 0s 53us/step - loss: 0.0619 - acc: 0.9860 - val_loss: 0.2760 - val_acc: 0.9150\n",
      "Epoch 27/40\n",
      "4800/4800 [==============================] - 0s 54us/step - loss: 0.0609 - acc: 0.9850 - val_loss: 0.2757 - val_acc: 0.9117\n",
      "Epoch 28/40\n",
      "4800/4800 [==============================] - 0s 52us/step - loss: 0.0559 - acc: 0.9869 - val_loss: 0.2670 - val_acc: 0.9150\n",
      "Epoch 29/40\n",
      "4800/4800 [==============================] - 0s 55us/step - loss: 0.0558 - acc: 0.9873 - val_loss: 0.2696 - val_acc: 0.9133\n",
      "Epoch 30/40\n",
      "4800/4800 [==============================] - 0s 56us/step - loss: 0.0606 - acc: 0.9858 - val_loss: 0.2825 - val_acc: 0.9100\n",
      "Epoch 31/40\n",
      "4800/4800 [==============================] - 0s 57us/step - loss: 0.0614 - acc: 0.9862 - val_loss: 0.2884 - val_acc: 0.9150\n",
      "Epoch 32/40\n",
      "4800/4800 [==============================] - 0s 56us/step - loss: 0.0574 - acc: 0.9869 - val_loss: 0.2804 - val_acc: 0.9167\n",
      "Epoch 33/40\n",
      "4800/4800 [==============================] - 0s 69us/step - loss: 0.0544 - acc: 0.9867 - val_loss: 0.2823 - val_acc: 0.9133\n",
      "Epoch 34/40\n",
      "4800/4800 [==============================] - 0s 66us/step - loss: 0.0538 - acc: 0.9867 - val_loss: 0.2706 - val_acc: 0.9150\n",
      "Epoch 35/40\n",
      "4800/4800 [==============================] - 0s 55us/step - loss: 0.0515 - acc: 0.9885 - val_loss: 0.2733 - val_acc: 0.9158\n",
      "Epoch 36/40\n",
      "4800/4800 [==============================] - 0s 65us/step - loss: 0.0514 - acc: 0.9881 - val_loss: 0.2720 - val_acc: 0.9217\n",
      "Epoch 37/40\n",
      "4800/4800 [==============================] - 0s 63us/step - loss: 0.0479 - acc: 0.9890 - val_loss: 0.2642 - val_acc: 0.9192\n",
      "Epoch 38/40\n",
      "4800/4800 [==============================] - 0s 56us/step - loss: 0.0484 - acc: 0.9883 - val_loss: 0.2700 - val_acc: 0.9183\n",
      "Epoch 39/40\n",
      "4800/4800 [==============================] - 0s 55us/step - loss: 0.0503 - acc: 0.9871 - val_loss: 0.2740 - val_acc: 0.9192\n",
      "Epoch 40/40\n",
      "4800/4800 [==============================] - 0s 58us/step - loss: 0.0493 - acc: 0.9885 - val_loss: 0.2731 - val_acc: 0.9150\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e30a570748>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(\n",
    "    X_train, \n",
    "    y_train_hot,\n",
    "    batch_size=512, \n",
    "    epochs=40,\n",
    "    validation_data=(X_val, y_val_hot),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvNet: LeNet-5 Architecture( Val_acc ~ 96.67%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"LeNet.png\"\n",
    "     alt=\"Markdown Monster icon\"\n",
    "     style=\"float: left; margin-right: 10px;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rechape X_train, X_val\n",
    "X_train_r = np.reshape(X_train, [-1,28,28,1])\n",
    "X_val_r   = np.reshape(X_val, [-1,28,28,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "def create_CNN():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters=6, kernel_size=(5,5), padding='same', activation=\"relu\", input_shape=(28,28,1)))\n",
    "    model.add(MaxPooling2D())\n",
    "    model.add(Conv2D(filters=16, kernel_size=(5,5), padding='same', activation=\"relu\"))\n",
    "    model.add(MaxPooling2D())\n",
    "    \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "    \n",
    "    return model    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0917 16:35:28.619404  7224 deprecation_wrapper.py:119] From c:\\users\\blender\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 28, 28, 6)         156       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 14, 14, 6)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 14, 14, 16)        2416      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 7, 7, 16)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                2570      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 206,102\n",
      "Trainable params: 206,102\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#describe model\n",
    "model2 = create_CNN()\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile( loss='categorical_crossentropy', optimizer=keras.optimizers.Adamax(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4800 samples, validate on 1200 samples\n",
      "Epoch 1/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 1.7075 - acc: 0.5048 - val_loss: 1.1304 - val_acc: 0.7500\n",
      "Epoch 2/50\n",
      "4800/4800 [==============================] - 4s 788us/step - loss: 0.8990 - acc: 0.8198 - val_loss: 0.7445 - val_acc: 0.8350\n",
      "Epoch 3/50\n",
      "4800/4800 [==============================] - 4s 845us/step - loss: 0.6117 - acc: 0.8869 - val_loss: 0.5415 - val_acc: 0.8875\n",
      "Epoch 4/50\n",
      "4800/4800 [==============================] - 4s 821us/step - loss: 0.4605 - acc: 0.9060 - val_loss: 0.4286 - val_acc: 0.9150\n",
      "Epoch 5/50\n",
      "4800/4800 [==============================] - 4s 834us/step - loss: 0.3667 - acc: 0.9246 - val_loss: 0.3639 - val_acc: 0.9275\n",
      "Epoch 6/50\n",
      "4800/4800 [==============================] - 4s 801us/step - loss: 0.3091 - acc: 0.9377 - val_loss: 0.3170 - val_acc: 0.9342\n",
      "Epoch 7/50\n",
      "4800/4800 [==============================] - 4s 934us/step - loss: 0.2664 - acc: 0.9506 - val_loss: 0.2840 - val_acc: 0.9342\n",
      "Epoch 8/50\n",
      "4800/4800 [==============================] - 5s 944us/step - loss: 0.2336 - acc: 0.9554 - val_loss: 0.2548 - val_acc: 0.9467\n",
      "Epoch 9/50\n",
      "4800/4800 [==============================] - 4s 891us/step - loss: 0.2072 - acc: 0.9602 - val_loss: 0.2338 - val_acc: 0.9508\n",
      "Epoch 10/50\n",
      "4800/4800 [==============================] - 4s 843us/step - loss: 0.1847 - acc: 0.9644 - val_loss: 0.2232 - val_acc: 0.9517\n",
      "Epoch 11/50\n",
      "4800/4800 [==============================] - 4s 839us/step - loss: 0.1666 - acc: 0.9677 - val_loss: 0.2070 - val_acc: 0.9517\n",
      "Epoch 12/50\n",
      "4800/4800 [==============================] - 4s 849us/step - loss: 0.1509 - acc: 0.9723 - val_loss: 0.1964 - val_acc: 0.9550\n",
      "Epoch 13/50\n",
      "4800/4800 [==============================] - 4s 869us/step - loss: 0.1385 - acc: 0.9750 - val_loss: 0.1897 - val_acc: 0.9558\n",
      "Epoch 14/50\n",
      "4800/4800 [==============================] - 4s 923us/step - loss: 0.1259 - acc: 0.9788 - val_loss: 0.1827 - val_acc: 0.9558\n",
      "Epoch 15/50\n",
      "4800/4800 [==============================] - 4s 895us/step - loss: 0.1160 - acc: 0.9808 - val_loss: 0.1729 - val_acc: 0.9575\n",
      "Epoch 16/50\n",
      "4800/4800 [==============================] - 4s 915us/step - loss: 0.1072 - acc: 0.9827 - val_loss: 0.1669 - val_acc: 0.9583\n",
      "Epoch 17/50\n",
      "4800/4800 [==============================] - 4s 921us/step - loss: 0.1010 - acc: 0.9835 - val_loss: 0.1640 - val_acc: 0.9592\n",
      "Epoch 18/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0944 - acc: 0.9850 - val_loss: 0.1562 - val_acc: 0.9642\n",
      "Epoch 19/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0887 - acc: 0.9860 - val_loss: 0.1532 - val_acc: 0.9625\n",
      "Epoch 20/50\n",
      "4800/4800 [==============================] - 5s 957us/step - loss: 0.0838 - acc: 0.9875 - val_loss: 0.1488 - val_acc: 0.9608\n",
      "Epoch 21/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0792 - acc: 0.9879 - val_loss: 0.1484 - val_acc: 0.9600\n",
      "Epoch 22/50\n",
      "4800/4800 [==============================] - 5s 959us/step - loss: 0.0753 - acc: 0.9890 - val_loss: 0.1450 - val_acc: 0.9617\n",
      "Epoch 23/50\n",
      "4800/4800 [==============================] - 5s 969us/step - loss: 0.0709 - acc: 0.9904 - val_loss: 0.1414 - val_acc: 0.9617\n",
      "Epoch 24/50\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.0677 - acc: 0.9902 - val_loss: 0.1384 - val_acc: 0.9633\n",
      "Epoch 25/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0645 - acc: 0.9908 - val_loss: 0.1375 - val_acc: 0.9633\n",
      "Epoch 26/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0616 - acc: 0.9917 - val_loss: 0.1370 - val_acc: 0.9625\n",
      "Epoch 27/50\n",
      "4800/4800 [==============================] - 5s 990us/step - loss: 0.0590 - acc: 0.9921 - val_loss: 0.1333 - val_acc: 0.9633\n",
      "Epoch 28/50\n",
      "4800/4800 [==============================] - 5s 993us/step - loss: 0.0565 - acc: 0.9931 - val_loss: 0.1330 - val_acc: 0.9642\n",
      "Epoch 29/50\n",
      "4800/4800 [==============================] - 5s 996us/step - loss: 0.0545 - acc: 0.9929 - val_loss: 0.1311 - val_acc: 0.9642\n",
      "Epoch 30/50\n",
      "4800/4800 [==============================] - 5s 1000us/step - loss: 0.0527 - acc: 0.9933 - val_loss: 0.1300 - val_acc: 0.9650\n",
      "Epoch 31/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0507 - acc: 0.9935 - val_loss: 0.1275 - val_acc: 0.9642\n",
      "Epoch 32/50\n",
      "4800/4800 [==============================] - 5s 993us/step - loss: 0.0489 - acc: 0.9933 - val_loss: 0.1276 - val_acc: 0.9642\n",
      "Epoch 33/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0474 - acc: 0.9938 - val_loss: 0.1261 - val_acc: 0.9633\n",
      "Epoch 34/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0458 - acc: 0.9942 - val_loss: 0.1256 - val_acc: 0.9633\n",
      "Epoch 35/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0445 - acc: 0.9942 - val_loss: 0.1234 - val_acc: 0.9642\n",
      "Epoch 36/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0433 - acc: 0.9944 - val_loss: 0.1223 - val_acc: 0.9658\n",
      "Epoch 37/50\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.0421 - acc: 0.9944 - val_loss: 0.1227 - val_acc: 0.9650\n",
      "Epoch 38/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0410 - acc: 0.9944 - val_loss: 0.1216 - val_acc: 0.9642\n",
      "Epoch 39/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0400 - acc: 0.9944 - val_loss: 0.1194 - val_acc: 0.9667\n",
      "Epoch 40/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0389 - acc: 0.9944 - val_loss: 0.1196 - val_acc: 0.9650\n",
      "Epoch 41/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0379 - acc: 0.9944 - val_loss: 0.1191 - val_acc: 0.9650\n",
      "Epoch 42/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0371 - acc: 0.9946 - val_loss: 0.1170 - val_acc: 0.9683\n",
      "Epoch 43/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0361 - acc: 0.9946 - val_loss: 0.1165 - val_acc: 0.9675\n",
      "Epoch 44/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0352 - acc: 0.9946 - val_loss: 0.1160 - val_acc: 0.9667\n",
      "Epoch 45/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0343 - acc: 0.9948 - val_loss: 0.1158 - val_acc: 0.9675\n",
      "Epoch 46/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0337 - acc: 0.9950 - val_loss: 0.1148 - val_acc: 0.9683\n",
      "Epoch 47/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0328 - acc: 0.9948 - val_loss: 0.1141 - val_acc: 0.9675\n",
      "Epoch 48/50\n",
      "4800/4800 [==============================] - 5s 1ms/step - loss: 0.0321 - acc: 0.9952 - val_loss: 0.1128 - val_acc: 0.9675\n",
      "Epoch 49/50\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.0313 - acc: 0.9956 - val_loss: 0.1135 - val_acc: 0.9667\n",
      "Epoch 50/50\n",
      "4800/4800 [==============================] - 6s 1ms/step - loss: 0.0305 - acc: 0.9956 - val_loss: 0.1126 - val_acc: 0.9667\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e30a9cd2b0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model\n",
    "model2.fit(\n",
    "    X_train_r, y_train_hot,\n",
    "    batch_size=512, \n",
    "    epochs=50,\n",
    "    validation_data=(X_val_r, y_val_hot)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
